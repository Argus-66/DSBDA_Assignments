{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8504012a-1fce-4202-a837-78a4ee6a3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Install and Import Required Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7cf8fe-294d-4461-bc8f-d8b8960290ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/argus/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/argus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/argus/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/argus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/argus/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Download NLTK resources (only first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7becb65-6420-4ee8-a8b7-80bbc42d12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Social_Network_Ads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dfa6d2b-b5a0-4967-b867-088039a455bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "    User ID  Gender  Age  EstimatedSalary  Purchased\n",
      "0  15624510    Male   19            19000          0\n",
      "1  15810944    Male   35            20000          0\n",
      "2  15668575  Female   26            43000          0\n",
      "3  15603246  Female   27            57000          0\n",
      "4  15804002    Male   19            76000          0\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows to understand the structure\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed51132-8a43-4845-b8ec-0141ef5d6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the dataset:\n",
      "['User ID', 'Gender', 'Age', 'EstimatedSalary', 'Purchased']\n"
     ]
    }
   ],
   "source": [
    "# Check the column names to find text columns\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bbe7714-e282-462a-9cb2-a60f5362d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample text descriptions:\n",
      "Sample 1: Male user aged 19 with estimated salary of 19000 did not purchase the product despite seeing social media advertisements.\n",
      "Sample 2: Male user aged 35 with estimated salary of 20000 did not purchase the product despite seeing social media advertisements.\n",
      "Sample 3: Female user aged 26 with estimated salary of 43000 did not purchase the product despite seeing social media advertisements.\n",
      "Sample 4: Female user aged 27 with estimated salary of 57000 did not purchase the product despite seeing social media advertisements.\n",
      "Sample 5: Male user aged 19 with estimated salary of 76000 did not purchase the product despite seeing social media advertisements.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create Sample Text Data from Dataset\n",
    "# Since Social_Network_Ads doesn't have much text, we'll create text descriptions\n",
    "# Step 3.1: Generate text descriptions based on user demographics\n",
    "\n",
    "def generate_description(row):\n",
    "    gender = row['Gender']\n",
    "    age = row['Age']\n",
    "    salary = row['EstimatedSalary']\n",
    "    purchased = row['Purchased']\n",
    "    \n",
    "    description = f\"{gender} user aged {age} with estimated salary of {salary} \"\n",
    "    if purchased == 1:\n",
    "        description += \"purchased the product after seeing social media advertisements.\"\n",
    "    else:\n",
    "        description += \"did not purchase the product despite seeing social media advertisements.\"\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Step 3.2: Create new column with text descriptions\n",
    "df['Description'] = df.apply(generate_description, axis=1)\n",
    "\n",
    "# Step 3.3: Display some sample descriptions\n",
    "print(\"\\nSample text descriptions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}: {df['Description'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49156bfd-b4bc-4aa3-9220-a0e86e66dd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing preprocessing function on the first description:\n",
      "Original text: Male user aged 19 with estimated salary of 19000 did not purchase the product despite seeing social media advertisements.\n",
      "After lowercase: male user aged 19 with estimated salary of 19000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '19000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '19000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('19', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('19000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['male', 'user', 'age', '19', 'estim', 'salari', '19000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/argus/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# Step 4: Define Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Step 4.1: Convert to lowercase\n",
    "    text = text.lower()\n",
    "    print(f\"After lowercase: {text}\")\n",
    "    \n",
    "    # Step 4.2: Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    print(f\"After tokenization: {tokens}\")\n",
    "    \n",
    "    # Step 4.3: Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    print(f\"After removing punctuation: {tokens}\")\n",
    "    \n",
    "    # Step 4.4: Remove Stop Words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    print(f\"After removing stop words: {filtered_tokens}\")\n",
    "    \n",
    "    # Step 4.5: POS Tagging\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    print(f\"POS tags: {pos_tags}\")\n",
    "    \n",
    "    # Step 4.6: Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    print(f\"After stemming: {stemmed_tokens}\")\n",
    "    \n",
    "    # Step 4.7: Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    print(f\"After lemmatization: {lemmatized_tokens}\")\n",
    "    \n",
    "    return {\n",
    "        'tokens': tokens,\n",
    "        'filtered': filtered_tokens,\n",
    "        'pos_tags': pos_tags,\n",
    "        'stemmed': stemmed_tokens,\n",
    "        'lemmatized': lemmatized_tokens\n",
    "    }\n",
    "\n",
    "# Step 4.8: Test the preprocessing function on one sample\n",
    "print(\"\\nTesting preprocessing function on the first description:\")\n",
    "sample_text = df['Description'].iloc[0]\n",
    "print(f\"Original text: {sample_text}\")\n",
    "result = preprocess_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d0ba4ce-f009-4357-942b-4a149a763000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 1 ---\n",
      "Original: Male user aged 19 with estimated salary of 19000 did not purchase the product despite seeing social media advertisements.\n",
      "After lowercase: male user aged 19 with estimated salary of 19000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '19000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '19000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('19', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('19000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['male', 'user', 'age', '19', 'estim', 'salari', '19000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Summary:\n",
      "Original Tokens: 19 tokens\n",
      "After Stop Removal: 14 tokens\n",
      "Parts of Speech: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('19', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('19000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After Stemming: ['male', 'user', 'age', '19', 'estim', 'salari', '19000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After Lemmatization: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "--- Sample 2 ---\n",
      "Original: Male user aged 35 with estimated salary of 20000 did not purchase the product despite seeing social media advertisements.\n",
      "After lowercase: male user aged 35 with estimated salary of 20000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['male', 'user', 'aged', '35', 'with', 'estimated', 'salary', 'of', '20000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['male', 'user', 'aged', '35', 'with', 'estimated', 'salary', 'of', '20000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['male', 'user', 'aged', '35', 'estimated', 'salary', '20000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('35', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('20000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['male', 'user', 'age', '35', 'estim', 'salari', '20000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['male', 'user', 'aged', '35', 'estimated', 'salary', '20000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Summary:\n",
      "Original Tokens: 19 tokens\n",
      "After Stop Removal: 14 tokens\n",
      "Parts of Speech: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('35', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('20000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After Stemming: ['male', 'user', 'age', '35', 'estim', 'salari', '20000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After Lemmatization: ['male', 'user', 'aged', '35', 'estimated', 'salary', '20000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "--- Sample 3 ---\n",
      "Original: Female user aged 26 with estimated salary of 43000 did not purchase the product despite seeing social media advertisements.\n",
      "After lowercase: female user aged 26 with estimated salary of 43000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['female', 'user', 'aged', '26', 'with', 'estimated', 'salary', 'of', '43000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['female', 'user', 'aged', '26', 'with', 'estimated', 'salary', 'of', '43000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['female', 'user', 'aged', '26', 'estimated', 'salary', '43000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('female', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('26', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('43000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['femal', 'user', 'age', '26', 'estim', 'salari', '43000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['female', 'user', 'aged', '26', 'estimated', 'salary', '43000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Summary:\n",
      "Original Tokens: 19 tokens\n",
      "After Stop Removal: 14 tokens\n",
      "Parts of Speech: [('female', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('26', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('43000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After Stemming: ['femal', 'user', 'age', '26', 'estim', 'salari', '43000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After Lemmatization: ['female', 'user', 'aged', '26', 'estimated', 'salary', '43000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Process Multiple Text Samples\n",
    "# Step 5.1: Select a subset of descriptions to process\n",
    "sample_descriptions = df['Description'].head(3).tolist()\n",
    "\n",
    "# Step 5.2: Process each description\n",
    "for i, text in enumerate(sample_descriptions):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Original: {text}\")\n",
    "    result = preprocess_text(text)\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(\"\\nSummary:\")\n",
    "    print(f\"Original Tokens: {len(result['tokens'])} tokens\")\n",
    "    print(f\"After Stop Removal: {len(result['filtered'])} tokens\")\n",
    "    print(f\"Parts of Speech: {result['pos_tags']}\")\n",
    "    print(f\"After Stemming: {result['stemmed']}\")\n",
    "    print(f\"After Lemmatization: {result['lemmatized']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "548cb361-d851-4a85-8937-18bdf4bc4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Step 6.1: Preparing corpus...\n",
      "\n",
      "Preprocessing document 1...\n",
      "After lowercase: male user aged 19 with estimated salary of 19000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '19000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '19000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('19', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('19000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['male', 'user', 'age', '19', 'estim', 'salari', '19000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['male', 'user', 'aged', '19', 'estimated', 'salary', '19000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Preprocessing document 2...\n",
      "After lowercase: male user aged 35 with estimated salary of 20000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['male', 'user', 'aged', '35', 'with', 'estimated', 'salary', 'of', '20000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['male', 'user', 'aged', '35', 'with', 'estimated', 'salary', 'of', '20000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['male', 'user', 'aged', '35', 'estimated', 'salary', '20000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('35', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('20000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['male', 'user', 'age', '35', 'estim', 'salari', '20000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['male', 'user', 'aged', '35', 'estimated', 'salary', '20000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Preprocessing document 3...\n",
      "After lowercase: female user aged 26 with estimated salary of 43000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['female', 'user', 'aged', '26', 'with', 'estimated', 'salary', 'of', '43000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['female', 'user', 'aged', '26', 'with', 'estimated', 'salary', 'of', '43000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['female', 'user', 'aged', '26', 'estimated', 'salary', '43000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('female', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('26', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('43000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['femal', 'user', 'age', '26', 'estim', 'salari', '43000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['female', 'user', 'aged', '26', 'estimated', 'salary', '43000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Preprocessing document 4...\n",
      "After lowercase: female user aged 27 with estimated salary of 57000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['female', 'user', 'aged', '27', 'with', 'estimated', 'salary', 'of', '57000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['female', 'user', 'aged', '27', 'with', 'estimated', 'salary', 'of', '57000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['female', 'user', 'aged', '27', 'estimated', 'salary', '57000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('female', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('27', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('57000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['femal', 'user', 'age', '27', 'estim', 'salari', '57000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['female', 'user', 'aged', '27', 'estimated', 'salary', '57000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "Preprocessing document 5...\n",
      "After lowercase: male user aged 19 with estimated salary of 76000 did not purchase the product despite seeing social media advertisements.\n",
      "After tokenization: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '76000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements', '.']\n",
      "After removing punctuation: ['male', 'user', 'aged', '19', 'with', 'estimated', 'salary', 'of', '76000', 'did', 'not', 'purchase', 'the', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "After removing stop words: ['male', 'user', 'aged', '19', 'estimated', 'salary', '76000', 'purchase', 'product', 'despite', 'seeing', 'social', 'media', 'advertisements']\n",
      "POS tags: [('male', 'JJ'), ('user', 'NN'), ('aged', 'VBD'), ('19', 'CD'), ('estimated', 'VBN'), ('salary', 'JJ'), ('76000', 'CD'), ('purchase', 'NN'), ('product', 'NN'), ('despite', 'IN'), ('seeing', 'VBG'), ('social', 'JJ'), ('media', 'NNS'), ('advertisements', 'NNS')]\n",
      "After stemming: ['male', 'user', 'age', '19', 'estim', 'salari', '76000', 'purchas', 'product', 'despit', 'see', 'social', 'media', 'advertis']\n",
      "After lemmatization: ['male', 'user', 'aged', '19', 'estimated', 'salary', '76000', 'purchase', 'product', 'despite', 'seeing', 'social', 'medium', 'advertisement']\n",
      "\n",
      "# Step 6.2: Applying TF-IDF Vectorization...\n",
      "\n",
      "Vocabulary (unique words): ['19' '19000' '20000' '26' '27' '35' '43000' '57000' '76000'\n",
      " 'advertisement' 'aged' 'despite' 'estimated' 'female' 'male' 'medium'\n",
      " 'product' 'purchase' 'salary' 'seeing' 'social' 'user']\n",
      "\n",
      "TF-IDF Matrix shape: (5, 22)\n",
      "TF-IDF Matrix (first 5 rows):\n",
      "[[0.37628974 0.46640144 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.22224278 0.22224278 0.22224278\n",
      "  0.22224278 0.         0.31235448 0.22224278 0.22224278 0.22224278\n",
      "  0.22224278 0.22224278 0.22224278 0.22224278]\n",
      " [0.         0.         0.44964171 0.         0.         0.44964171\n",
      "  0.         0.         0.         0.21425669 0.21425669 0.21425669\n",
      "  0.21425669 0.         0.3011303  0.21425669 0.21425669 0.21425669\n",
      "  0.21425669 0.21425669 0.21425669 0.21425669]\n",
      " [0.         0.         0.         0.44071482 0.         0.\n",
      "  0.44071482 0.         0.         0.21000298 0.21000298 0.21000298\n",
      "  0.21000298 0.35556595 0.         0.21000298 0.21000298 0.21000298\n",
      "  0.21000298 0.21000298 0.21000298 0.21000298]\n",
      " [0.         0.         0.         0.         0.44071482 0.\n",
      "  0.         0.44071482 0.         0.21000298 0.21000298 0.21000298\n",
      "  0.21000298 0.35556595 0.         0.21000298 0.21000298 0.21000298\n",
      "  0.21000298 0.21000298 0.21000298 0.21000298]\n",
      " [0.37628974 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.46640144 0.22224278 0.22224278 0.22224278\n",
      "  0.22224278 0.         0.31235448 0.22224278 0.22224278 0.22224278\n",
      "  0.22224278 0.22224278 0.22224278 0.22224278]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 6: Perform TF-IDF Vectorization\n",
    "# Step 6.1: Prepare corpus by preprocessing all descriptions\n",
    "print(\"\\n# Step 6.1: Preparing corpus...\")\n",
    "preprocessed_docs = []\n",
    "# Using only 5 descriptions to keep output manageable\n",
    "for idx, text in enumerate(df['Description'].head(5)):\n",
    "    print(f\"\\nPreprocessing document {idx+1}...\")\n",
    "    preprocessed = preprocess_text(text)\n",
    "    preprocessed_docs.append(' '.join(preprocessed['lemmatized']))\n",
    "\n",
    "# Step 6.2: Apply TF-IDF Vectorization\n",
    "print(\"\\n# Step 6.2: Applying TF-IDF Vectorization...\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Step 6.3: Get feature names (vocabulary)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nVocabulary (unique words): {feature_names}\")\n",
    "\n",
    "# Step 6.4: Display TF-IDF matrix\n",
    "print(\"\\nTF-IDF Matrix shape:\", tfidf_matrix.shape)\n",
    "print(\"TF-IDF Matrix (first 5 rows):\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27bbf6af-6fa4-40d4-9411-946e3ca35d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Step 7.1: Analyzing TF-IDF scores...\n",
      "\n",
      "Document 1 TF-IDF scores:\n",
      "19000: 0.4664\n",
      "19: 0.3763\n",
      "male: 0.3124\n",
      "user: 0.2222\n",
      "aged: 0.2222\n",
      "estimated: 0.2222\n",
      "salary: 0.2222\n",
      "purchase: 0.2222\n",
      "product: 0.2222\n",
      "despite: 0.2222\n",
      "seeing: 0.2222\n",
      "social: 0.2222\n",
      "medium: 0.2222\n",
      "advertisement: 0.2222\n",
      "\n",
      "Document 2 TF-IDF scores:\n",
      "35: 0.4496\n",
      "20000: 0.4496\n",
      "male: 0.3011\n",
      "user: 0.2143\n",
      "aged: 0.2143\n",
      "estimated: 0.2143\n",
      "salary: 0.2143\n",
      "purchase: 0.2143\n",
      "product: 0.2143\n",
      "despite: 0.2143\n",
      "seeing: 0.2143\n",
      "social: 0.2143\n",
      "medium: 0.2143\n",
      "advertisement: 0.2143\n",
      "\n",
      "Document 3 TF-IDF scores:\n",
      "26: 0.4407\n",
      "43000: 0.4407\n",
      "female: 0.3556\n",
      "user: 0.2100\n",
      "aged: 0.2100\n",
      "estimated: 0.2100\n",
      "salary: 0.2100\n",
      "purchase: 0.2100\n",
      "product: 0.2100\n",
      "despite: 0.2100\n",
      "seeing: 0.2100\n",
      "social: 0.2100\n",
      "medium: 0.2100\n",
      "advertisement: 0.2100\n",
      "\n",
      "Document 4 TF-IDF scores:\n",
      "27: 0.4407\n",
      "57000: 0.4407\n",
      "female: 0.3556\n",
      "user: 0.2100\n",
      "aged: 0.2100\n",
      "estimated: 0.2100\n",
      "salary: 0.2100\n",
      "purchase: 0.2100\n",
      "product: 0.2100\n",
      "despite: 0.2100\n",
      "seeing: 0.2100\n",
      "social: 0.2100\n",
      "medium: 0.2100\n",
      "advertisement: 0.2100\n",
      "\n",
      "Document 5 TF-IDF scores:\n",
      "76000: 0.4664\n",
      "19: 0.3763\n",
      "male: 0.3124\n",
      "user: 0.2222\n",
      "aged: 0.2222\n",
      "estimated: 0.2222\n",
      "salary: 0.2222\n",
      "purchase: 0.2222\n",
      "product: 0.2222\n",
      "despite: 0.2222\n",
      "seeing: 0.2222\n",
      "social: 0.2222\n",
      "medium: 0.2222\n",
      "advertisement: 0.2222\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Analyze TF-IDF Scores\n",
    "# Step 7.1: Show TF-IDF scores for each word in each document\n",
    "print(\"\\n# Step 7.1: Analyzing TF-IDF scores...\")\n",
    "for i, doc in enumerate(preprocessed_docs):\n",
    "    print(f\"\\nDocument {i+1} TF-IDF scores:\")\n",
    "    feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "    for word_idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{feature_names[word_idx]}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c783510-340e-4f13-bde8-d156d47f198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Step 8.1: Applying Bag of Words...\n",
      "\n",
      "Count Vectorizer Vocabulary: ['19' '19000' '20000' '26' '27' '35' '43000' '57000' '76000'\n",
      " 'advertisement' 'aged' 'despite' 'estimated' 'female' 'male' 'medium'\n",
      " 'product' 'purchase' 'salary' 'seeing' 'social' 'user']\n",
      "\n",
      "Count Matrix (Bag of Words):\n",
      "[[1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1]\n",
      " [0 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      " [0 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Apply Bag of Words (Count Vectorization)\n",
    "# Step 8.1: Use CountVectorizer on the preprocessed documents\n",
    "print(\"\\n# Step 8.1: Applying Bag of Words...\")\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Step 8.2: Get feature names\n",
    "count_features = count_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nCount Vectorizer Vocabulary: {count_features}\")\n",
    "\n",
    "# Step 8.3: Display Count matrix\n",
    "print(\"\\nCount Matrix (Bag of Words):\")\n",
    "print(count_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ffd70fa-8b02-4a1b-890d-d75002e88051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Step 9.1: Analyzing word frequencies...\n",
      "\n",
      "Document 1 word counts:\n",
      "male: 1\n",
      "user: 1\n",
      "aged: 1\n",
      "19: 1\n",
      "estimated: 1\n",
      "salary: 1\n",
      "19000: 1\n",
      "purchase: 1\n",
      "product: 1\n",
      "despite: 1\n",
      "seeing: 1\n",
      "social: 1\n",
      "medium: 1\n",
      "advertisement: 1\n",
      "\n",
      "Document 2 word counts:\n",
      "male: 1\n",
      "user: 1\n",
      "aged: 1\n",
      "estimated: 1\n",
      "salary: 1\n",
      "purchase: 1\n",
      "product: 1\n",
      "despite: 1\n",
      "seeing: 1\n",
      "social: 1\n",
      "medium: 1\n",
      "advertisement: 1\n",
      "35: 1\n",
      "20000: 1\n",
      "\n",
      "Document 3 word counts:\n",
      "user: 1\n",
      "aged: 1\n",
      "estimated: 1\n",
      "salary: 1\n",
      "purchase: 1\n",
      "product: 1\n",
      "despite: 1\n",
      "seeing: 1\n",
      "social: 1\n",
      "medium: 1\n",
      "advertisement: 1\n",
      "female: 1\n",
      "26: 1\n",
      "43000: 1\n",
      "\n",
      "Document 4 word counts:\n",
      "user: 1\n",
      "aged: 1\n",
      "estimated: 1\n",
      "salary: 1\n",
      "purchase: 1\n",
      "product: 1\n",
      "despite: 1\n",
      "seeing: 1\n",
      "social: 1\n",
      "medium: 1\n",
      "advertisement: 1\n",
      "female: 1\n",
      "27: 1\n",
      "57000: 1\n",
      "\n",
      "Document 5 word counts:\n",
      "male: 1\n",
      "user: 1\n",
      "aged: 1\n",
      "19: 1\n",
      "estimated: 1\n",
      "salary: 1\n",
      "purchase: 1\n",
      "product: 1\n",
      "despite: 1\n",
      "seeing: 1\n",
      "social: 1\n",
      "medium: 1\n",
      "advertisement: 1\n",
      "76000: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Analyze Word Frequencies\n",
    "# Step 9.1: Show word counts for each document\n",
    "print(\"\\n# Step 9.1: Analyzing word frequencies...\")\n",
    "for i, doc in enumerate(preprocessed_docs):\n",
    "    print(f\"\\nDocument {i+1} word counts:\")\n",
    "    feature_index = count_matrix[i,:].nonzero()[1]\n",
    "    count_scores = zip(feature_index, [count_matrix[i, x] for x in feature_index])\n",
    "    for word_idx, count in sorted(count_scores, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{count_features[word_idx]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7681076c-3aa7-4e3e-b176-e5d02893ccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Step 10.1: Calculating document similarity...\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "[[1.         0.61784642 0.51338811 0.51338811 0.7824697 ]\n",
      " [0.61784642 1.         0.49493996 0.49493996 0.61784642]\n",
      " [0.51338811 0.49493996 1.         0.61154089 0.51338811]\n",
      " [0.51338811 0.49493996 0.61154089 1.         0.51338811]\n",
      " [0.7824697  0.61784642 0.51338811 0.51338811 1.        ]]\n",
      "\n",
      "# Step 10.2: Most similar document pairs:\n",
      "Similarity between Document 1 and Document 2: 0.6178\n",
      "Similarity between Document 1 and Document 3: 0.5134\n",
      "Similarity between Document 1 and Document 4: 0.5134\n",
      "Similarity between Document 1 and Document 5: 0.7825\n",
      "Similarity between Document 2 and Document 3: 0.4949\n",
      "Similarity between Document 2 and Document 4: 0.4949\n",
      "Similarity between Document 2 and Document 5: 0.6178\n",
      "Similarity between Document 3 and Document 4: 0.6115\n",
      "Similarity between Document 3 and Document 5: 0.5134\n",
      "Similarity between Document 4 and Document 5: 0.5134\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Compare Documents Using TF-IDF\n",
    "# Step 10.1: Calculate document similarity using cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"\\n# Step 10.1: Calculating document similarity...\")\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(cosine_sim)\n",
    "\n",
    "# Step 10.2: Identify most similar documents\n",
    "print(\"\\n# Step 10.2: Most similar document pairs:\")\n",
    "for i in range(len(preprocessed_docs)):\n",
    "    for j in range(i+1, len(preprocessed_docs)):\n",
    "        print(f\"Similarity between Document {i+1} and Document {j+1}: {cosine_sim[i][j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e83a3c-9bdd-4982-92c6-a1bd5aaea43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
